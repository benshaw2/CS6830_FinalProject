{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac64314",
   "metadata": {},
   "source": [
    "## As a first proof of concept, we optimize the linear model y=mx+b subject to m>0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e19027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4798d27e",
   "metadata": {},
   "source": [
    "## Here are the functions \"from the math\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb5411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSELoss(p,X,y): # This is the 1-dimensional MSE loss in the original coordinates.\n",
    "    N = len(y)\n",
    "    term = 1/N*sum([(y[i]-p[0]*X[i]-p[1])**2 for i in range(N)])\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e8150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myGrad1(p,X,y): # This assumes that the original loss function is the MSE.\n",
    "    N = len(y)\n",
    "    s1 = sum([np.exp(p[0])*X[i]**2 + p[1]*X[i] + X[i]**2 - X[i]*y[i] for i in range(N)])\n",
    "    s2 = sum([np.exp(p[0])*X[i]+X[i]+p[1]-y[i] for i in range(N)])\n",
    "    v1 = 2*np.exp(-p[0])*s1\n",
    "    v2 = 2*s2\n",
    "    ans = np.array([v1/N,v2/N])\n",
    "    return ans # the components of the gradient of the MSE in our coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myUpdate1(p,v):\n",
    "    \n",
    "    def vMove(p2,v2,t):\n",
    "        # initial v velocity: c3\n",
    "        # initial v position: c4\n",
    "        c3 = (-v2)\n",
    "        c4 = p2\n",
    "        return c3*t + c4\n",
    "    \n",
    "    def uMove(p1,v1,t):\n",
    "        # initial u-velocity: c1/c2\n",
    "        # initial u position: ln(c2)\n",
    "        # p1 = ln(c2); c2=exp(p1)\n",
    "        # v1 = c1/c2; c1 = exp(p1)*v1\n",
    "        c1 = np.exp(p1)*(-v1)\n",
    "        #print(c1)\n",
    "        c2 = np.exp(p1)\n",
    "        #print(c2)\n",
    "        return np.log(c1*t+c2)\n",
    "    \n",
    "    ans = np.array([uMove(p[0],v[0],1.0), vMove(p[1],v[1],1.0)])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d601eb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradUpdate1(p,X,y,eta):\n",
    "    \n",
    "    gradP = myGrad1(p,X,y)\n",
    "    #print(gradP)\n",
    "    gradPlr = eta*gradP\n",
    "    #print(gradPlr)\n",
    "    newP = myUpdate1(p,gradPlr)\n",
    "    return newP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48a638",
   "metadata": {},
   "source": [
    "## Now we need to implement Gradient Descent (in batches?) Here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e83b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X - X data points\n",
    "y - y data points\n",
    "pt - Initial point to start at\n",
    "Update - Update function to get a new point\n",
    "epochs - Number of iterations to narrow down the best point\n",
    "batchsize - The size of the sliced dataset to use in the update function\n",
    "lr - The learning rate\n",
    "\"\"\"\n",
    "def SGD(X, y, pt, Update, epochs, batchsize=64, lr=0.01):\n",
    "\tpts = [pt]\n",
    "\txBatch = X\n",
    "\tyBatch = y\n",
    "\tfor _ in range(epochs):\n",
    "\t\tif len(X) > batchsize:\n",
    "\t\t\t# We want to get a random list of integers to get a batch\n",
    "\t\t\txBatch = []\n",
    "\t\t\tyBatch = []\n",
    "\t\t\trandList = random.sample(range(0, len(X)), batchsize)\n",
    "\t\t\tfor index in randList:\n",
    "\t\t\t\txBatch.append(X[index])\n",
    "\t\t\t\tyBatch.append(y[index])\n",
    "\t\t# Call passed in function TODO: Only will work with gradupdate1, need a way to have it work with multiple.\n",
    "\t\tpt = Update(pt,xBatch,yBatch,lr)\n",
    "\t\tpts.extend([pt]) # Track the path in the original coordinates.\n",
    "\treturn pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89be60c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc4e4a8f",
   "metadata": {},
   "source": [
    "## Sample with dummy data: make sure the \"math checks out.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e0ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = np.linspace(0,10,100)\n",
    "ytest = [x+1 for x in Xtest] # So the model is y= 1*x+1.\n",
    "print(len(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pt = [2,0] # Initialize (new coordinates). This is (exp(2)+1, 0) in the original coordinates.\n",
    "pts = [pt]\n",
    "epochs = 1200 # How long to train for?\n",
    "myeta = 0.01 # Careful with the learning rate: if it's too big, you can run into a \"bad point!\"\n",
    "for i in range(epochs):\n",
    "    pt = gradUpdate1(pt,Xtest,ytest,myeta)\n",
    "    pts.extend([pt]) # Track the path in the original coordinates.\n",
    "\"\"\"\n",
    "pts = SGD(Xtest, ytest, [2,0], gradUpdate1, 1200)\n",
    "opts = [np.array([np.exp(p[0])+1,p[1]]) for p in pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa32c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [MSELoss(p,Xtest,ytest) for p in opts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daec91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the loss for the first few iterations drop.\n",
    "plt.plot(losses[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss continues to drop.\n",
    "plt.plot(losses[10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd58d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is our approximate solution:\n",
    "print(opts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e8920",
   "metadata": {},
   "source": [
    "# Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e91542",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/avocado.csv')\n",
    "df = df[['AveragePrice', 'Total Volume']].copy()\n",
    "\n",
    "plt.scatter(x=df['Total Volume'], y=df['AveragePrice'])\n",
    "plt.xlabel(\"Volume Sold\")\n",
    "plt.ylabel(\"Average Price\")\n",
    "plt.title(\"Avocados Sold vs Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31020968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df = pd.read_csv('data/avocado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d48f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edddf497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
